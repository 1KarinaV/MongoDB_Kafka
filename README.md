# Брокер сообщений Kafka как трасса данных
## MongoDb

Необходимо мигрировать данные из источника (MongoDB база) в целевое хранилище (тоже MongoDB база) с минимальным временем простоя сервисов которые работают с базой источником и базой целевого назначения.
База источник - source, а целевая база - target.

Обеспечить миграцию данных из source базы в target базу с сохранением структурности данных, используя Docker образы.
Т.е. есть в source базе бд с именем test, в бд с именем test существует коллекция users, коллекция users имеет структуру документа
```plain
{
  "_id": Number(),
  "firstname": String(),
  "lastname": String(),
  "age": Number(),
  "email": String()
}
```

Необходимо получить структурность из source db/test/users в target db/test/users с сохранением всех данных

Для решения задачи необходимо использовать:
kafka в однонодовой инсталляции
mongodb Source в однонодовой инсталляции
mongodb Target в однонодовой инсталляции
kafka-connect  в однонодовой инсталляции

## Mongodb и несоответствие типов данных (Задача с одной звёздочкой).
Задача такая же как и с mongoDB, но теперь у вас есть проблема...
Разработчики сперва имели схему MongoDb в коллекции users  такого вида
```plain
{
  "_id": Number(),
  "firstname": String(),
  "lastname": String(),
  "age": Number(),
  "email": String()
}
```


Но потом пришли другие разработчики и что-то поменялось, теперь схема такая
'''
{
"_id": String(),
"firstname": String(),
"lastname": String(),
"age": LongInt(),
"email": String(),
"sex": String()
}
"'''

Вот тут есть проблема с типами данных, которые необходимо поправить при вставке в базу
Для этого можно использовать либо Schema Registry, либо самопальный воркер который будет читать из топиков kafka и сам всё складывать в target DB.


## ClickHouse (Задача с двумя звёздами или тремя)
Есть kafka, в неё кто-то или что-то кидает данные, мы не знаем кто или что такое делает, для нас это магия.
Что нам известно, это формат данных который приходит в топик kafka и есть clickhouse.
Бизнес говорит "Хочу все данные из топика metrics в kafka складывать автоматически в ClickHouse в табличку metrics и потом что-то там считать"
Понимаем, что данные метрические, какие-то значения и их может быть много, много столбиков будет в ClickHouse и ClickHouse нам хорошо подходит для этого дела.
Магия как была  с MongoDB не прокатит, тут мы просто читаем из Kafka и кладём в ClickHouse.
Так вот, чтобы сотворить такую штуку, надо сам ClickHouse подключить к kafka.
да-да, просто коннекторов найти нельзя. Есть в ClickHouse движок kafka engine который специально был создан и используется для связности между kafka и ClickHouse.
Тут мы просто шагаем в доку по ClickHouse (она на русском и подробная) и просто поднимаем решение.
